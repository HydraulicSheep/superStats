{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification - In or Out?\n",
    "by [HydraulicSheep](https://github.com/HydraulicSheep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References: \n",
    " http://cs229.stanford.edu/notes2020spring/cs229-notes1.pdf **[1]**\n",
    " \n",
    " https://archive.ics.uci.edu/ml/datasets/Wine+Quality **[2]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In nature, a very intuitive concept is that of groups. Objects with similar characteristics can be lumped together, despite slight differences, to reveal meaningful relationships and rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics, classification is simply the problem of assigning datapoints to groups. For example, classifying over the famous [MNIST Digit Dataset](http://yann.lecun.com/exdb/mnist/) can identify handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Linear Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we'll be looking at for this task is a collection of characteristics for a [variety of wines](https://archive.ics.uci.edu/ml/datasets/Wine+Quality) **[2]**. Some of them are red and others are white. The goal is to best classify the red and white wines so that, theoretically, we can successfully predict these types. The specific machine learning technique we'll use for this is called '**Logistic Regression**'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "whiteWines = pandas.read_csv('winequality-white.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        6  \n",
       "1      9.5        6  \n",
       "2     10.1        6  \n",
       "3      9.9        6  \n",
       "4      9.9        6  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whiteWines.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "redWines = pandas.read_csv('winequality-red.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redWines.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our data is imported and contains a number of (hopefully) useful factors for classifying wines. However, reading the dataset information, the quality is an 'output variable'. As we aren't training to assess the wine's quality, lte's just remove that from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4               0.7          0.0             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  \n",
       "0      9.4  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redWines = redWines.drop(columns=['quality'])\n",
    "redWines.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.001</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density   pH  sulphates  alcohol  \n",
       "0                 45.0                 170.0    1.001  3.0       0.45      8.8  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whiteWines = whiteWines.drop(columns=['quality'])\n",
    "whiteWines.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything's established, let's start writing some tools for our classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following from Linear Regression, Logistic Regression is a very natural system.\n",
    "We need to find a function that best maps any point in the training data to the correct class - white wine or red wine.\n",
    "\n",
    "Let's define one class as having the y-value '1' and the other as having y-value '0'.\n",
    "\n",
    "So, for the function to produce valid outputs, we need a function that maps all possible X values to a y-value between 0 and 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few functions that do this, but one of them is the logistic/sigmoid curve:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f(x) = \\frac{1}{1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for a linear classifier, we assume that the class is given by a linear combination of a data-point's features. i.e. a weighted sum $= \\theta_{0} + \\theta_{1}x_1 + \\theta_{2}x_2 + ... + \\theta_{m}x_m $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expressing this in matrix form, with matrices $ \\theta $ and X, we get: $$ s(x) =  \\theta^{T}X $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to confine this between 0 and 1, we take f(s(x)) $$ h(x) = \\frac{1}{1+e^{\\theta^{T}X}}$$ \n",
    "where h(x) is called our 'hypothesis function'.\n",
    "\n",
    "Let's define this as a function of theta and X:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a matrix of weights, theta, and training points, x\n",
    "def h(theta, x):\n",
    "    theta_tr = theta.transpose()\n",
    "    prod = np.matmul(theta,x)\n",
    "    return 1/(1+math.exp(-prod))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this on some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([1,2,3,4])\n",
    "x = np.array([1,-2,3,-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5397868702434395e-05"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h(theta,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, from our classifier definitions, we know the following probabilities [1]:\n",
    "$$ Pr(y = 1 |x;\\theta ) = h(x) $$\n",
    "$$ Pr(y = 0 |x;\\theta ) = 1- h(x) $$\n",
    "\n",
    "And, from these, we can describe the likelihood $ L(\\theta) $ of the parameters, describing the probability that this training outcome was observed if the true parameter values are as given. The log likelihood is given by $ l(\\theta)  = log(L(\\theta)) $.\n",
    "\n",
    "To find the optimal set of parameters, we seek $\\theta$ that maximises $L(\\theta)$. Clearly, this will also maximise $l(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, by some algebra [1], for logistic regression, the log likelihood function is given by:\n",
    "$$ l(\\theta)=\\sum_{i=1}^{n}y^{(i)}log\\, h(x^{(i)})\\, +\\, (1-y^{(i)})log(1-h(x^{(i)}))  $$\n",
    "Let's implement this as a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(theta, X, Y):\n",
    "    total = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        total+=Y[i]*np.log(h(theta,np.array(X[i])))+(1-Y[i])*np.log(1-h(theta,np.array(X[i])))\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, let's test a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-47.001022854253044"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = np.array([1,3])\n",
    "X = np.array([[1,4],[2,5],[6,8]])\n",
    "Y = np.array([1,0,0,1])\n",
    "log_likelihood(theta,X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we just need a process to **maximise the log likelihood** function. One such method is gradient ascent. \n",
    "\n",
    "This involves increasing each value of theta such that the cost function will locally increase the most - i.e. changing each $ \\theta_j $ in the **direction of the log-likelihood's partial derivative with respect to $ \\theta_j $**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, per some calculus [1], this partial derivative for any $ \\theta_j $ is given by: $$ \\frac{\\partial l(\\theta)}{\\partial \\theta_j} = (y-h(x))x_j$$\n",
    "\n",
    "Now, we want to change theta by some 'rate' proportional to this partial derivative. The constant of proportionality is called the 'learning rate'.\n",
    "\n",
    "Let's define this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(theta,rate,x,y):\n",
    "    theta = theta + rate*(y-h(theta,x))*x\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we can get to training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is simply the process of running a number of iterations of this step process. To show how this works for a small set of parameters, let's generate two classes of data randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1]\n",
      "[-7.91779223  1.21474517  0.44424733]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Let's try training on some placeholder data\n",
    "theta = np.array([1,1,1])\n",
    "red = []\n",
    "blue = []\n",
    "\n",
    "# Let's generate some points\n",
    "\n",
    "# Red has class 1\n",
    "# Blue has class 0\n",
    "# Let's distribute red points around 20\n",
    "# Let's distribute blue points around 0\n",
    "\n",
    "for i in range(100):\n",
    "    x = np.random.normal(20,2) # Sampling from Gaussian distribution around 20\n",
    "    y = np.random.normal(20,2)\n",
    "    red.append([1,x,y])\n",
    "  \n",
    "red = np.array(red)\n",
    "\n",
    "for i in range(100):\n",
    "    x = np.random.normal(0,2) # Sampling from Gaussian distribution around 0\n",
    "    y = np.random.normal(0,2)\n",
    "    blue.append([1,x,y])\n",
    "    \n",
    "blue = np.array(blue)\n",
    "\n",
    "rate = 0.1\n",
    "print(theta)\n",
    "STEPS = 10000\n",
    "for i in range(STEPS):\n",
    "    for j in range(X.shape[0]):      \n",
    "        theta = step(theta,rate,red[j],1)\n",
    "        theta = step(theta,rate,blue[j],0)\n",
    "print(theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have a trained model. Let's take a look at our decision boundary.\n",
    "The decision boundary give points with a hypothesis value of 0.5 - they fall between two classes. So solve: $$ h(x) = \\frac{1}{1+e^{\\theta^{T}X}} = \\frac{1}{2} $$ $$ 2 = 1 + e^{\\theta^{T}X} $$ $$ e^{\\theta^{T}X} = 1 $$ $$ \\theta^{T}X = 0 $$\n",
    "\n",
    "So we solve for when the weighted sum is zero and (for y = $ x_2 $) we get: $$ \\theta_2y + \\theta_1x_1 + \\theta_0 = 0 $$ $$ y = -\\frac{\\theta_1}{\\theta_2}x_1 -\\frac{\\theta_0}{\\theta_2} $$\n",
    "\n",
    "If we plot this, we get the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Xaxis = np.arange(-20., 20., 0.2)\n",
    "line = []\n",
    "for i in Xaxis:\n",
    "    line.append(-theta[1]*i/theta[2] - theta[0]/theta[2])\n",
    "plt.plot(Xaxis, line)\n",
    "\n",
    "x = [red[i][1] for i in range(len(red))]\n",
    "y = [red[i][2] for i in range(len(red))]\n",
    "plt.plot(x, y, 'r+')\n",
    "\n",
    "x2 = [blue[i][1] for i in range(len(blue))]\n",
    "y2 = [blue[i][2] for i in range(len(blue))]\n",
    "plt.plot(x2, y2, 'b+')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Our two groups have been clearly separated by the line and the line is fairly far from both groups. \n",
    "\n",
    "**NOTE**: *This process was repeated a few times to get a good fit for the line. I didn't want to spend a lot of time fiddling with hyperparameters and working with large numbers of samples which may have been required given this data's truly random nature.*\n",
    "\n",
    "Let's check what the hypothesis is at x=10 - between the two means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9998287359499409"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h(theta,[1,10,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, this is fairly close to 0.5. In reality, per the random variable definitions, the point (x,y)=(10,10) is equally likely to be a member of either class. \n",
    "\n",
    "We can test this model more thoroughly by running some already-classified test data through. This **must be different to the training data** or else the results obtained will not reflect its predictive ability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99942\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "TESTS = 100000\n",
    "for i in range(TESTS):\n",
    "    redx = np.random.normal(20,2)\n",
    "    redy = np.random.normal(20,2)\n",
    "    bluex = np.random.normal(0,2)\n",
    "    bluey = np.random.normal(0,2)\n",
    "    r = [1,redx,redy]\n",
    "    b = [1,bluex,bluey]\n",
    "    \n",
    "    if h(theta,r)>=0.5:\n",
    "        total+=1\n",
    "    if h(theta,b)<0.5:\n",
    "        total+=1\n",
    "        \n",
    "print(total/(TESTS*2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our model predicts the classes of sampled points with 99.9% accuracy. This makes sense because the standard deviation of our distributions are fairly low so overlap between the Gaussian distributions should be minimal\n",
    "\n",
    "NOTE: The modelling of this completely random data isn't very sound probabilistically - we aren't modelling Gaussian error but instead a variable that has Gaussian distribution. However, the effect of these is very similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wine Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all our tools set up, let's repeat this for the wine data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
       "       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
       "       'pH', 'sulphates', 'alcohol'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whiteInfo = np.array(whiteWines)\n",
    "redInfo = np.array(redWines)\n",
    "\n",
    "# Adds a column of 1s to the data - needed for constant parameter\n",
    "whiteInfo = np.insert(whiteInfo,0,1,axis=1)\n",
    "redInfo = np.insert(redInfo,0,1,axis=1)\n",
    "\n",
    "whiteWines.columns # Available data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our model (optimise parameters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[ 144.3804847   -26.96398501 -385.25260103   38.06873149   10.47426644\n",
      "  -89.43356776   -1.97519062    2.43600757  134.20025537  -46.48858353\n",
      " -293.14478752   20.1141584 ]\n"
     ]
    }
   ],
   "source": [
    "theta = np.ones(len(redInfo[0]))\n",
    "rate = 0.01\n",
    "print(theta)\n",
    "STEPS = 10000\n",
    "for i in range(STEPS):\n",
    "    for j in range(1000):      \n",
    "        # Processes a white wine - Class 1\n",
    "        theta = step(theta,rate,whiteInfo[j],1)\n",
    "        # Processes a red wine - Class 0\n",
    "        theta = step(theta,rate,redInfo[j],0)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the previous example, we can't plot the classification boundary for this dataset because it is represented by a hyperplane in n-dimensional space where n is the number of characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(theta,start_i,end_i):\n",
    "    total = 0\n",
    "    for i in range(start_i,end_i):\n",
    "    # Process white wines\n",
    "        if (h(theta,whiteInfo[i])>=0.5):\n",
    "            total+=1\n",
    "    # Process red wines\n",
    "        if (h(theta,redInfo[i])<0.5):\n",
    "            total+=1\n",
    "    print(total/(2*(end_i-start_i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the first 1000 entries for training so let's test on the other datapoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9682804674457429\n"
     ]
    }
   ],
   "source": [
    "test(theta,1000,1599)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not Bad. Some simple iteration and a little bit of calculus allow us to relate a linear combination of characteristics with a datapoint's correct class. And 96.8% correct is fairly good; certainly better than an uninformed human could do from such a large set of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we test on our training data, we get a higher (**inaccurate**) result because the training specifically optimised over this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.975\n"
     ]
    }
   ],
   "source": [
    "test(theta,0,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like linear regression, logistic regression is a very useful technique which arises from **fairly simple principles**. By the **power of classification**, data patterns can be modelled and valuable predictions made. And, as with linear regression, these standard techniques shouldn't be immediately discarded for more advanced (and more complicated) ones.\n",
    "______________\n",
    "Stay tuned to my ***Github*** - [HydraulicSheep](https://github.com/HydraulicSheep) - for explorations of more great ***Game Theory*** and ***Statistics*** content."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
