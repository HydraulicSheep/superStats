{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Approximation with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One significant use of neural networks is in **approximating** the values of **continuous functions**. Particularly in areas like Deep Reinforcement Learning over continuous/high-dimensional state-spaces. \n",
    "\n",
    "For example, **Reinforcement Learning** algorithms like **Deep Q Learning** or **Fitted Value Iteration** can use neural networks to approximate Q functions, policies or value functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] Cybenko, G. (1989). Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, and Systems, 2(4), 303â€“314. doi:10.1007/bf02551274 \n",
    "\n",
    "[2] https://en.wikipedia.org/wiki/Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Universal Approximation Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of Artificial Neural Networks, universal approximation theorems concern the ability of feedforward networks to approximate certain classes of functions. Let us take a look at one specific class of neural networks - those with a single hidden layer of neurons with sigmoidal activation functions.\n",
    "\n",
    "A sigmoidal\n",
    "\n",
    "One specific theorem is as following [1]:\n",
    "\n",
    "\n",
    "Now, for our practical purposes, this is more of a novelty; the theorem says nothing about the **required network size** or whether approximations can be obtained by **any specific learning algorithm**. \n",
    "\n",
    "Other such theorems build up support for the **existence** of neural networks as function approximators but let's look at some data to examine their practical performance under **standard learning algorithms** (i.e. backpropagation), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Parabola"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a basic parabola:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x113cd5040>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.linspace(-50, 50, 100)\n",
    "y = x**2\n",
    "\n",
    "plt.scatter(x, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try with a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\[Tangent\\] A Neural Network from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our target problem is fairly basic and doesn't require high-performance, let's attempt making a neural network from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    \n",
    "    def __init__(self, size):\n",
    "        self.neurons = []\n",
    "        self.size = size\n",
    "        self.next_layer = None\n",
    "        self.prev_layer = None\n",
    "        \n",
    "    def output(self):\n",
    "        \n",
    "        result = []\n",
    "        \n",
    "        for neuron in self.neurons:\n",
    "            result.append(neuron.value)\n",
    "            \n",
    "        print(result)\n",
    "                \n",
    "\n",
    "class DenseLayer(Layer):\n",
    "    \n",
    "    \n",
    "    def __init__(self, neuron_count, activation_function, prev_layer):\n",
    "        \n",
    "        super().__init__(neuron_count)\n",
    "        \n",
    "        \n",
    "        # Handle connection with previous layer\n",
    "        self.prev_layer = prev_layer\n",
    "        prev_layer.next_layer = self\n",
    "        \n",
    "        # Creates Neurons\n",
    "        for i in range(neuron_count):\n",
    "            \n",
    "            new_neuron = Neuron(activation_function, np.random.rand(prev_layer.size), np.random.rand())\n",
    "            \n",
    "            self.neurons.append(new_neuron)\n",
    "            \n",
    "            # Connects this neuron in a dense fashion to the previous layer\n",
    "            for neuron in prev_layer.neurons:\n",
    "                \n",
    "                new_neuron.back_conns.append(neuron)\n",
    "                neuron.forward_conns.append(new_neuron)\n",
    "                \n",
    "    def forward_propagate(self):\n",
    "\n",
    "        for i in self.neurons:\n",
    "            i.process()\n",
    "            i.send_forward()\n",
    "\n",
    "        if self.next_layer:\n",
    "            self.next_layer.forward_propagate()\n",
    "            \n",
    "    \n",
    "                \n",
    "class InputLayer(Layer):\n",
    "    \n",
    "    def __init__(self, input_size):\n",
    "        \n",
    "        super().__init__(input_size)\n",
    "        \n",
    "        for i in range(input_size):\n",
    "            self.neurons.append(Neuron())\n",
    "    \n",
    "    def forward_propagate(self, input):\n",
    "        \n",
    "             \n",
    "        if len(input) != len(self.neurons):\n",
    "            print('Input Dimension Error!')\n",
    "            return\n",
    "        \n",
    "        for i in range(len(input)):\n",
    "            self.neurons[i].value = input[i]\n",
    "            self.neurons[i].send_forward()\n",
    "            \n",
    "        if self.next_layer:\n",
    "            self.next_layer.forward_propagate()\n",
    "        \n",
    "\n",
    "class Neuron:\n",
    "    \n",
    "    def __init__(self, activation_function=None, weight=None, bias=None):\n",
    "        \n",
    "        self.forward_conns = []\n",
    "        self.back_conns = []\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "        self.input = []\n",
    "        self.activation_function = activation_function\n",
    "        self.value = None\n",
    "    \n",
    "    def process(self):\n",
    "        \n",
    "        arr = np.array(self.input)\n",
    "        \n",
    "        if len(self.input) == 0:\n",
    "            print('Error! Uninitialised Node.')\n",
    "            return\n",
    "        \n",
    "        if self.activation_function == None:\n",
    "            print('No Activation Function')\n",
    "            return\n",
    "        \n",
    "        if arr.shape != self.weight.shape:\n",
    "            print('Invalid Weight')\n",
    "            print(self.weight) \n",
    "            print(arr)\n",
    "            return\n",
    "        \n",
    "        if self.bias == None:\n",
    "            print('Invalid Bias')\n",
    "            return\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        result = np.dot(arr,self.weight) + self.bias\n",
    "        self.value = self.activation_function(result)\n",
    "        \n",
    "            \n",
    "    def send_forward(self):\n",
    "        for neuron in self.forward_conns:\n",
    "            neuron.input.append(self.value)\n",
    "        \n",
    "    \n",
    "def sigmoid(x):\n",
    "    return (1/(1+exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer = InputLayer(4)\n",
    "prev_layer = first_layer\n",
    "for i in range(10):\n",
    "    layer = DenseLayer(8, sigmoid, prev_layer)\n",
    "    prev_layer = layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer.forward_propagate([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "first_layer.output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9792439504432007, 0.9806157732132809, 0.9944067349044886, 0.9901381298429864, 0.9982045589365947, 0.9914067166454952, 0.9949419980887186, 0.9904242290749925]\n"
     ]
    }
   ],
   "source": [
    "layer.output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we've achieved basic forward propagation with arbitrary activation functions between dense layers, let's consider what our cost function could look like for function approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're performing a regression-style problem so, like with linear regression, let's try squared error:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's outline a derivation (using the Wikipedia [2] approach for weights but including the bias calculations):\n",
    "\n",
    "$$ Let\\ \\sigma(x)=\\frac{1}{1+e^{-x}}\\ be\\ the\\ sigmoid\\ function$$\n",
    "$$ Let\\ L(y,\\hat{y}) = (y-\\hat{y})^2\\ be\\ our\\ squared\\ error\\ loss\\ function\\ where\\ \\hat{y}\\ is\\ the\\ network's\\ estimate\\ and\\ y\\ is\\ the\\ true\\ value $$\n",
    "$$ Let\\ w\\ and\\ b\\ represent\\ weights\\ and\\ biases\\ respectively$$\n",
    "The output of some neuron j is given by:\n",
    "$$ o_j = \\sigma((\\sum_{k=1}^{n}w_{kj}o_{k}) + b_j)$$\n",
    "\n",
    "Now:\n",
    "\n",
    "$$ \\frac{d\\sigma (x)}{dx}= \\frac{-(-e^{-x})}{(1+e^{-x})^2}= \\frac{1+e^{-x}-1}{(1+e^{-x})^2}=\\sigma(x)(1-\\sigma(x))$$\n",
    "\n",
    "Let's now calculate the derivative of the loss function with respect to the weights and biases:\n",
    "\n",
    "First, the weights (using the chain rule for partial derivatives):\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial w_{ij}} = \\frac{\\partial L}{\\partial o_{j}}\\frac{\\partial o_{j}}{\\partial w_{ij}} = \\frac{\\partial L}{\\partial o_{j}}\\frac{\\partial o_{j}}{\\partial\\ linear_{j}}\\frac{\\partial\\ linear_{j}}{\\partial w_{ij}}\\quad where\\ linear_{j}\\ is\\ the\\ linear\\ input\\ to\\ the\\ \\sigma\\ function\\ for\\ neuron\\ j\\quad (1)$$\n",
    "\n",
    "$$ \\frac{\\partial\\ linear_{j}}{\\partial w_{ij}} =  \\frac{\\partial}{\\partial w_{ij}}((\\sum_{k=1}^{n}w_{kj}o_{k}) + b_j) =  \\frac{\\partial}{\\partial w_{ij}}w_{ij}o_{i} = o_i\\quad (2)$$\n",
    "\n",
    "$$ \\frac{\\partial o_j}{\\partial\\ linear_{j}} = \\frac{\\partial\\ \\sigma(linear_{j})}{\\partial\\ linear_{j}} = \\sigma(linear_{j})(1-\\sigma(linear_{j}))\\quad (shown\\ above)\\quad (3)$$\n",
    "\n",
    "Secondly, the biases (using the chain rule for partial derivatives):\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial b_{j}} = \\frac{\\partial E}{\\partial o_{j}}\\frac{\\partial o_{j}}{\\partial b_{j}} = \\frac{\\partial E}{\\partial o_{j}}\\frac{\\partial o_{j}}{\\partial\\ linear_{j}}\\frac{\\partial\\ linear_{j}}{\\partial b_{j}}\\quad (4)$$\n",
    "\n",
    "$$ \\frac{\\partial\\ linear_{j}}{\\partial b_{j}} =  \\frac{\\partial}{\\partial b_{j}}((\\sum_{k=1}^{n}w_{kj}o_{k}) + b_j) =  1\\quad (5)$$\n",
    "\n",
    "\n",
    "Now, here comes the genius of backpropagation:\n",
    "\n",
    "In the last (output) layer - a single node for our network,   $ o_j = \\hat{y} $\n",
    "\n",
    "So: \n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial o_j} = \\frac{\\partial L}{\\partial \\hat{y}} = \\frac{\\partial }{\\partial \\hat{y}}(\\frac{1}{2}(y-\\hat{y})^2) = \\hat{y} - y\\quad (6)$$\n",
    "\n",
    "Now, if we consider j being a neuron in any other layer:\n",
    "\n",
    "L is still a function of $ o_j $ because L is dependent on all the neurons receiving input from j:\n",
    "\n",
    "So:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial o_j} = \\frac{\\partial L(linear_a,linear_b, ...)}{\\partial o_j}\\ where\\ neurons\\ in\\ P=\\{ a,b,...\\}\\ receive\\ input from\\ neuron\\ j $$\n",
    "\n",
    "Now, the linear components are all functions of $o_j$ so are dependent on one another.\n",
    "\n",
    "Therefore we need to take the **total derivative** to find this:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial o_j} = \\sum_{p \\in P}(\\frac{\\partial L}{\\partial linear_p}\\frac{\\partial linear_p}{\\partial o_j}) = \\sum_{p \\in P}(\\frac{\\partial L}{\\partial o_p}\\frac{\\partial o_p}{\\partial linear_p}\\frac{\\partial linear_p}{\\partial o_j}) = \\sum_{p \\in P}(\\frac{\\partial L}{\\partial o_p}\\frac{\\partial o_p}{\\partial linear_p}w_{jp}) \\quad (7)$$\n",
    "\n",
    "So this term is recursively dependent on those of neurons in P.\n",
    "\n",
    "Now we have everything we need to find our partial derivatives for gradient descent.\n",
    "\n",
    "**For the weights:**\n",
    "\n",
    "Using Equations, 1,2,3,4,5:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_{ij}}  = \\frac{\\partial L}{\\partial o_{j}}\\frac{\\partial o_{j}}{\\partial\\ linear_{j}}\\frac{\\partial\\ linear_{j}}{\\partial w_{ij}} = \\frac{\\partial L}{\\partial o_{j}}\\frac{\\partial o_{j}}{\\partial\\ linear_{j}}o_i$$\n",
    "\n",
    "And we assign:\n",
    "\n",
    "$$\\delta_j = \\frac{\\partial L}{\\partial o_{j}}\\frac{\\partial o_{j}}{\\partial\\ linear_{j}} =   \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      (o_j-\\hat{y})o_j(1-o_j) & if\\ j\\ is\\ an\\ output\\ neuron\\\\\n",
    "      (\\sum_{p\\in P}(\\frac{\\partial L}{\\partial o_p}\\frac{\\partial o_p}{\\partial linear_p}w_{jp}))o_j(1-o_j) & if\\ j\\ is\\ an\\ inner\\ neuron\\\\\n",
    "\\end{array} \n",
    "\\right. \n",
    "= \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      (o_j-\\hat{y})o_j(1-o_j) & if\\ j\\ is\\ an\\ output\\ neuron\\\\\n",
    "      (\\sum_{p\\in P}(w_{jp}\\delta_{p}))o_j(1-o_j) & if\\ j\\ is\\ an\\ inner\\ neuron\\\\\n",
    "\\end{array} \n",
    "\\right. \n",
    "$$\n",
    "\n",
    "\n",
    "**Let's do the same with biases:**\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b_{j}}  = \\frac{\\partial L}{\\partial o_{j}}\\frac{\\partial o_{j}}{\\partial\\ linear_{j}}\\frac{\\partial\\ linear_{j}}{\\partial b_{j}} = \\frac{\\partial L}{\\partial o_{j}}\\frac{\\partial o_{j}}{\\partial\\ linear_{j}}$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_{ij}} = o_i\\delta_j $$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b_{j}} = \\delta_j $$\n",
    "\n",
    "Now we can perform gradient descent to improve our neural network over time by 'back-propagating' through the network and adjusting weights/biases proportional to the negative of these derivatives. The constand of proportionality is called the **'learning rate'**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    \n",
    "    def __init__(self, size):\n",
    "        self.neurons = []\n",
    "        self.size = size\n",
    "        self.next_layer = None\n",
    "        self.prev_layer = None\n",
    "        \n",
    "    def output(self):\n",
    "        \n",
    "        result = []\n",
    "        \n",
    "        for neuron in self.neurons:\n",
    "            result.append(neuron.result)\n",
    "            \n",
    "        print(result)\n",
    "        \n",
    "    def layer_info(self):\n",
    "        \n",
    "        if self.prev_layer == None:\n",
    "            print('Input Layer')\n",
    "            return\n",
    "        \n",
    "        for i in range(len(self.neurons)):\n",
    "            print('%d: %f' % (i, self.neurons[i].bias))\n",
    "            print(self.neurons[i].weight)\n",
    "                \n",
    "\n",
    "class DenseLayer(Layer):\n",
    "    \n",
    "    \n",
    "    def __init__(self, neuron_count, activation_function, prev_layer):\n",
    "        \n",
    "        super().__init__(neuron_count)\n",
    "        \n",
    "        \n",
    "        # Handle connection with previous layer\n",
    "        self.prev_layer = prev_layer\n",
    "        prev_layer.next_layer = self\n",
    "        \n",
    "        # Creates Neurons\n",
    "        for i in range(neuron_count):\n",
    "            \n",
    "            new_neuron = Neuron(activation_function, (np.random.rand(prev_layer.size)-0.5)*5, (np.random.rand()-0.5)*5)\n",
    "            \n",
    "            self.neurons.append(new_neuron)\n",
    "            \n",
    "            # Connects this neuron in a dense fashion to the previous layer\n",
    "            for neuron in prev_layer.neurons:\n",
    "                \n",
    "                new_neuron.back_conns.append(neuron)\n",
    "                neuron.forward_conns.append(new_neuron)\n",
    "                \n",
    "    def forward_propagate(self,layer):\n",
    "        \n",
    "        print('Layer ', layer)\n",
    "\n",
    "        for i in self.neurons:\n",
    "            i.process()\n",
    "            i.send_forward()\n",
    "\n",
    "        if self.next_layer:\n",
    "            self.next_layer.forward_propagate(layer+1)\n",
    "            \n",
    "    def back_propagate(self,true):\n",
    "        \n",
    "        for i in self.neurons:\n",
    "            i.backprop(true)\n",
    "            \n",
    "        self.prev_layer.back_propagate(true)\n",
    "            \n",
    "    \n",
    "                \n",
    "class InputLayer(Layer):\n",
    "    \n",
    "    def __init__(self, input_size):\n",
    "        \n",
    "        super().__init__(input_size)\n",
    "        \n",
    "        for i in range(input_size):\n",
    "            self.neurons.append(Neuron())\n",
    "    \n",
    "    def forward_propagate(self, input):\n",
    "        \n",
    "        \n",
    "        if len(input) != len(self.neurons):\n",
    "            print('Input Dimension Error!')\n",
    "            return\n",
    "        \n",
    "        for i in range(len(input)):\n",
    "            self.neurons[i].value = input[i]\n",
    "            self.neurons[i].send_forward()\n",
    "            \n",
    "        if self.next_layer:\n",
    "            self.next_layer.forward_propagate(1)\n",
    "            \n",
    "    # The input layer doesn't need to perform backprop\n",
    "    def back_propagate(self, true):\n",
    "        return\n",
    "        \n",
    "\n",
    "class Neuron:\n",
    "    \n",
    "    def __init__(self, activation_function=None, weight=None, bias=None):\n",
    "        \n",
    "        self.forward_conns = []\n",
    "        self.back_conns = []\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "        self.input = []\n",
    "        self.saved_input = None\n",
    "        self.activation_function = activation_function\n",
    "        self.value = None\n",
    "        self.delta = None\n",
    "    \n",
    "    def process(self):\n",
    "        arr = np.array(self.input)\n",
    "        \n",
    "        if len(self.input) == 0:\n",
    "            print('Error! Uninitialised Node.')\n",
    "            return\n",
    "        \n",
    "        if self.activation_function == None:\n",
    "            print('No Activation Function')\n",
    "            return\n",
    "        \n",
    "        if arr.shape != self.weight.shape:\n",
    "            print('Invalid Weight')\n",
    "            print(self.weight) \n",
    "            print(arr)\n",
    "            return\n",
    "        \n",
    "        if self.bias == None:\n",
    "            print('Invalid Bias')\n",
    "            return\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.result = np.dot(arr,self.weight) + self.bias\n",
    "        self.saved_input = self.input\n",
    "        self.input = []\n",
    "        self.value = self.activation_function(self.result)\n",
    "        \n",
    "    def backprop(self, true):\n",
    "        \n",
    "        \n",
    "        \n",
    "        if len(self.forward_conns) == 0:\n",
    "            self.delta = (self.value-true)*self.value*(1-self.value)\n",
    "            \n",
    "        else:\n",
    "            delta_sum = 0\n",
    "            for i in self.forward_conns:\n",
    "                delta_sum += i.weight_delta_product[self]\n",
    "            self.delta = delta_sum*self.value*(1-self.value)\n",
    "        \n",
    "        self.weight_delta_product = {}\n",
    "        for i in range(len(self.back_conns)):\n",
    "            self.weight_delta_product[self.back_conns[i]] = self.weight[i]*self.delta\n",
    "        \n",
    "        self.bias += -1*LEARNING_RATE*self.delta\n",
    "        self.weight = np.add(np.array(self.saved_input)*self.delta*-1*LEARNING_RATE,self.weight)\n",
    "            \n",
    "        \n",
    "            \n",
    "    def send_forward(self):\n",
    "        for neuron in self.forward_conns:\n",
    "            neuron.input.append(self.value)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer = InputLayer(1)\n",
    "prev_layer = first_layer\n",
    "for i in range(1):\n",
    "    layer = DenseLayer(8, sigmoid, prev_layer)\n",
    "    prev_layer = layer\n",
    "last_layer = DenseLayer(1,sigmoid,prev_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n",
      "Layer  1\n",
      "Layer  2\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    x = (np.random.rand()-0.5)*50\n",
    "    first_layer.forward_propagate([x])\n",
    "    last_layer.back_propagate(x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer  1\n",
      "Layer  2\n"
     ]
    }
   ],
   "source": [
    "first_layer.forward_propagate([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.331928079100754]\n"
     ]
    }
   ],
   "source": [
    "last_layer.output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: -1.715583\n",
      "[1.34510781]\n",
      "1: -2.068739\n",
      "[1.53496051]\n",
      "2: 2.196332\n",
      "[3.55271239]\n",
      "3: 0.066707\n",
      "[-0.3715733]\n",
      "4: -0.901708\n",
      "[-1.83584387]\n",
      "5: 0.884491\n",
      "[-1.34717331]\n",
      "6: 1.850817\n",
      "[-2.0868158]\n",
      "7: -0.212303\n",
      "[1.73425659]\n"
     ]
    }
   ],
   "source": [
    "first_layer.next_layer.layer_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: -1.155703\n",
      "[0.88509773 4.27982535 1.12331407 0.59011223 2.44880815 2.77732004\n",
      " 2.43062813 2.06525623]\n"
     ]
    }
   ],
   "source": [
    "last_layer.layer_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.3451078109184076, 1.5349605072949841, 3.5527123942122594, -0.3715733008667396, -1.8358438704842304, -1.347173312170075, -2.086815797904279, 1.734256588058802]\n"
     ]
    }
   ],
   "source": [
    "first_layer.next_layer.output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
